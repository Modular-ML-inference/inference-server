# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: basic-inference.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15\x62\x61sic-inference.proto\x12\x0f\x62\x61sic_inference\"(\n\x08Tensor32\x12\r\n\x05\x61rray\x18\x01 \x03(\x02\x12\r\n\x05shape\x18\x02 \x03(\x05\"N\n\x15\x42\x61sicInferenceRequest\x12\n\n\x02id\x18\x01 \x01(\x05\x12)\n\x06tensor\x18\x02 \x01(\x0b\x32\x19.basic_inference.Tensor32\"O\n\x16\x42\x61sicInferenceResponse\x12\n\n\x02id\x18\x01 \x01(\x05\x12)\n\x06tensor\x18\x02 \x01(\x0b\x32\x19.basic_inference.Tensor322y\n\x15\x42\x61sicInferenceService\x12`\n\x07predict\x12&.basic_inference.BasicInferenceRequest\x1a\'.basic_inference.BasicInferenceResponse\"\x00(\x01\x30\x01\x62\x06proto3')



_TENSOR32 = DESCRIPTOR.message_types_by_name['Tensor32']
_BASICINFERENCEREQUEST = DESCRIPTOR.message_types_by_name['BasicInferenceRequest']
_BASICINFERENCERESPONSE = DESCRIPTOR.message_types_by_name['BasicInferenceResponse']
Tensor32 = _reflection.GeneratedProtocolMessageType('Tensor32', (_message.Message,), {
  'DESCRIPTOR' : _TENSOR32,
  '__module__' : 'basic_inference_pb2'
  # @@protoc_insertion_point(class_scope:basic_inference.Tensor32)
  })
_sym_db.RegisterMessage(Tensor32)

BasicInferenceRequest = _reflection.GeneratedProtocolMessageType('BasicInferenceRequest', (_message.Message,), {
  'DESCRIPTOR' : _BASICINFERENCEREQUEST,
  '__module__' : 'basic_inference_pb2'
  # @@protoc_insertion_point(class_scope:basic_inference.BasicInferenceRequest)
  })
_sym_db.RegisterMessage(BasicInferenceRequest)

BasicInferenceResponse = _reflection.GeneratedProtocolMessageType('BasicInferenceResponse', (_message.Message,), {
  'DESCRIPTOR' : _BASICINFERENCERESPONSE,
  '__module__' : 'basic_inference_pb2'
  # @@protoc_insertion_point(class_scope:basic_inference.BasicInferenceResponse)
  })
_sym_db.RegisterMessage(BasicInferenceResponse)

_BASICINFERENCESERVICE = DESCRIPTOR.services_by_name['BasicInferenceService']
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _TENSOR32._serialized_start=42
  _TENSOR32._serialized_end=82
  _BASICINFERENCEREQUEST._serialized_start=84
  _BASICINFERENCEREQUEST._serialized_end=162
  _BASICINFERENCERESPONSE._serialized_start=164
  _BASICINFERENCERESPONSE._serialized_end=243
  _BASICINFERENCESERVICE._serialized_start=245
  _BASICINFERENCESERVICE._serialized_end=366
# @@protoc_insertion_point(module_scope)
